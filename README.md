# ğŸŒ¦ï¸ Full-Stack On-Premise Data Pipeline for IoT & Weather Data

## ğŸ“Œ Project Overview
This project implements a **full-stack, on-premise data engineering pipeline** for a hypothetical **Weather Analytics Company**.  
The pipeline ingests **real-time weather data**, generates **synthetic IoT and user data**, processes both **streaming and batch workloads**, and stores results across **data lake, relational, and analytical systems**.

The solution demonstrates **end-to-end data engineering skills**, including ingestion, processing, storage, orchestration, monitoring, and containerization.

---

## ğŸ§  Skills Gained from This Project
- Python  
- SQL  
- MongoDB  
- Apache Kafka  
- Apache Spark (Streaming & Batch)  
- Apache Hive  
- Apache Sqoop  
- Apache Airflow  
- Docker & Docker Compose  

---

## ğŸ­ Domain
**Weather Analytics & IoT Data Processing**

---

## ğŸ¯ Problem Statement
You are tasked with building an **on-premise data engineering pipeline** that:
- Ingests real-time weather data from a public API
- Generates synthetic data using Faker
- Handles both streaming and batch processing
- Stores data across Hive, MySQL, and Parquet
- Orchestrates workflows using Airflow
- (Optional) Containerizes the full system using Docker Compose

---

## ğŸ› ï¸ Technology Stack & Purpose

| Technology | Purpose |
|---------|---------|
| Apache Kafka | Real-time data ingestion |
| Apache Spark | Streaming & Batch Processing |
| Apache Hive | Data Lake / Analytical Tables |
| MySQL | Relational Storage |
| Apache Airflow | Workflow Orchestration |
| Docker Compose | Infrastructure Setup |
| Python | Data ingestion & ETL logic |

---

## ğŸ“‚ Project Structure
weather-data-pipeline/
â”‚
â”œâ”€â”€ airflow/
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â”œâ”€â”€ batch_etl_dag.py
â”‚   â”‚   â”œâ”€â”€ faker_csv_dag.py
â”‚   â”‚   â”œâ”€â”€ faker_mysql_dag.py
â”‚   â”‚   â””â”€â”€ weather_to_kafka_dag.py
â”‚
â”œâ”€â”€ kafka/
â”‚ â””â”€â”€ weather_to_kafka.py
â”‚
â”œâ”€â”€ faker/
â”‚   â”œâ”€â”€ generate_csv.py
â”‚   â””â”€â”€ insert_fake_mysql.py
â”‚
â”œâ”€â”€ spark/
â”‚ â”œâ”€â”€ streaming_kafka_to_parquet.py
â”‚ â”œâ”€â”€ batch_etl.py
â”‚ â””â”€â”€ last_etl_timestamp.txt
â”‚
â”œâ”€â”€ csv_parquet_storage/
â”‚   â”œâ”€â”€ fake_weather.csv
â”‚   â”‚
â”‚   â”œâ”€â”€ parquet_output/
â”‚   â”‚   â””â”€â”€ part-00000-6dd5899c-d930-479c-ac12-4dbb4f9808ba-c000.snappy.parquet
â”‚   â”‚
â”‚   â””â”€â”€ hive_final_table_export/
â”‚       â””â”€â”€ part-00000-6ddcee01-6085-4af2-acc9-894bd7b5b796-c000.csv
â”œâ”€â”€ hive/
â”‚ â””â”€â”€ create_hive_tables.sql
â”‚
â”œâ”€â”€ docker/
â”‚ â””â”€â”€ docker-compose.yml # Optional
â”‚
â””â”€â”€ README.md

---

## ğŸ”„ Pipeline Architecture Overview

### 1ï¸âƒ£ Ingestion Layer
#### Weather API âœ Kafka
- Fetches real-time weather data from **OpenWeatherMap API**
- Data is pushed to Kafka topic:
weather-topic

- Scheduled every minute using **Airflow DAG**

#### Faker âœ CSV
- Generates synthetic user weather logs (Name, City, Temperature)
- Writes CSV files every minute

#### Faker âœ MySQL
- Generates mock IoT sensor/device data
- Inserts records into MySQL table

---

### 2ï¸âƒ£ Processing Layer
#### Spark Streaming
- Consumes data from Kafka topic
- Writes processed output as **Parquet files**
- Trigger interval: **every 5 minutes**

#### Spark Batch ETL
- Reads:
- CSV files (Faker-generated)
- MySQL tables (sensor/device data)
- Performs transformations:
- Join
- Filter
- Select
- Loads results into:
- Hive Table (`final_table`)
- MySQL Table (`final_table`)

---

### 3ï¸âƒ£ Storage Layer
- **Parquet Files** â†’ Kafka streaming output
- **Hive Tables** â†’ Analytical data lake
- **MySQL Tables** â†’ Relational serving layer

---

### 4ï¸âƒ£ Orchestration
- **Apache Airflow** orchestrates:
- Weather API ingestion
- Kafka producers
- Spark batch ETL jobs

---

### 5ï¸âƒ£ Monitoring & Observability
- Pipeline health
- Job execution
- Resource utilization
- (Optional) Monitor Airflow, Spark, and Docker containers

---

## â–¶ï¸ How to Run the Project

### Prerequisites
- Python 3.x
- Apache Kafka
- Apache Spark
- Apache Hive
- MySQL
- Apache Airflow
- Docker & Docker Compose (optional)

---

### Step 1: Start Kafka Producer
bash
python kafka/weather_producer.py

Step 2: Run Faker Data Generators
bash
python faker/faker_to_csv.py
python faker/faker_to_mysql.py

Step 3: Start Airflow
Enable DAGs:
weather_to_kafka_dag
batch_etl_dag

Step 4: Run Spark Jobs
bash
spark-submit spark/streaming_kafka_to_parquet.py
spark-submit spark/batch_etl.py

ğŸ“Š Results & Deliverables
Each submission includes:
Project Code Repository

Airflow DAGs
weather_to_kafka_dag.py
batch_etl_dag.py

Spark Jobs
streaming_kafka_to_parquet.py
batch_etl.py

Docker Compose File (Optional)

Hive SQL Scripts

README.md (This file)

ğŸ‘¤ Author
Gayatri
Python Backend Engineer | Data Engineer
